{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWGTcOdTNdm0EFgWMqvxXc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickelodeon20/Test-Model-Performances/blob/main/ATCNet/ATCNet_BCI_IV2a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GuBKuD-jJyV"
      },
      "outputs": [],
      "source": [
        "# Authors: Lukas Gemein\n",
        "#          Hubert Banville\n",
        "#          Simon Brandt\n",
        "#          Daniel Wilson\n",
        "#\n",
        "# License: BSD (3-clause)\n",
        "!pip install braindecode # Install braindecode package\n",
        "from braindecode.datasets import MOABBDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subject_id = 3\n",
        "\n",
        "!pip install moabb\n",
        "!pip install --upgrade numpy==1.26.0\n",
        "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[subject_id])"
      ],
      "metadata": {
        "id": "hVKVvSgFn7k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy\n",
        "\n",
        "from numpy import multiply\n",
        "\n",
        "from braindecode.preprocessing import (Preprocessor,\n",
        "                                       exponential_moving_standardize,\n",
        "                                       preprocess)\n",
        "\n",
        "low_cut_hz = 4.  # low cut frequency for filtering\n",
        "high_cut_hz = 38.  # high cut frequency for filtering\n",
        "# Parameters for exponential moving standardization\n",
        "factor_new = 1e-3\n",
        "init_block_size = 1000\n",
        "# Factor to convert from V to uV\n",
        "factor = 1e6\n",
        "\n",
        "# Define a named function for volt-to-microvolt conversion\n",
        "def convert_to_microvolts(data, factor=factor):\n",
        "    return multiply(data, factor)\n",
        "\n",
        "preprocessors = [\n",
        "    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
        "    Preprocessor(convert_to_microvolts,  apply_on_array=True),  # Convert from V to uV using the named function\n",
        "    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
        "    Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n",
        "                 factor_new=factor_new, init_block_size=init_block_size)\n",
        "]\n",
        "\n",
        "# Transform the data\n",
        "preprocess(dataset, preprocessors, n_jobs=-1)"
      ],
      "metadata": {
        "id": "zz0Rc0UnoBVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from braindecode.preprocessing import create_windows_from_events\n",
        "\n",
        "trial_start_offset_seconds = -0.5\n",
        "# Extract sampling frequency, check that they are same in all datasets\n",
        "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
        "assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
        "# Calculate the trial start offset in samples.\n",
        "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
        "\n",
        "# Create windows using braindecode function for this. It needs parameters to define how\n",
        "# trials should be used.\n",
        "windows_dataset = create_windows_from_events(\n",
        "    dataset,\n",
        "    trial_start_offset_samples=trial_start_offset_samples,\n",
        "    trial_stop_offset_samples=0,\n",
        "    preload=True,\n",
        ")"
      ],
      "metadata": {
        "id": "z8qK7ojtoN6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from braindecode.datasets import BaseConcatDataset\n",
        "\n",
        "# Debugging function to check shapes of datasets\n",
        "def get_baseconcatdataset_shape(dataset):\n",
        "    \"\"\"\n",
        "    Gets the shape of a BaseConcatDataset.\n",
        "\n",
        "    Args:\n",
        "        dataset: The BaseConcatDataset object.\n",
        "\n",
        "    Returns:\n",
        "        A tuple representing the shape of the dataset: (num_examples, num_channels, num_timepoints).\n",
        "    \"\"\"\n",
        "\n",
        "    num_examples = len(dataset)\n",
        "\n",
        "    # If dataset is empty, return (num_examples, 0, 0)\n",
        "    if not dataset.datasets:\n",
        "        return (num_examples, 0, 0)\n",
        "\n",
        "    # Assume all datasets have the same shape, access shape of the first example\n",
        "    # of the first dataset\n",
        "    num_channels = dataset.datasets[0][0][0].shape[0]\n",
        "    num_timepoints = dataset.datasets[0][0][0].shape[1]\n",
        "\n",
        "    return (num_examples, num_channels, num_timepoints)\n",
        "\n",
        "\n",
        "\n",
        "# Split the dataset into training and validation set.\n",
        "\n",
        "splitted = windows_dataset.split('session')\n",
        "train_set = splitted['0train']  # Session train\n",
        "valid_set = splitted['1test']  # Session evaluation\n",
        "print(\"Windows_dataset before split: \" + str(get_baseconcatdataset_shape(windows_dataset)))\n",
        "print(\"------------------------------\")\n",
        "print(\"Valid_set before split: \" + str(get_baseconcatdataset_shape(valid_set)))\n",
        "print(\"------------------------------\")\n",
        "print(\"Train_set (not involved in split): \" + str(get_baseconcatdataset_shape(train_set)) + \"\\n\")\n",
        "\n",
        "# Get the list of datasets within valid_set\n",
        "valid_set_datasets = valid_set.datasets\n",
        "\n",
        "# Create new split indices within the valid_set\n",
        "n_valid_trials = len(valid_set_datasets)\n",
        "valid_subset1_ids = list(range(0, int(0.8 * n_valid_trials)))  # First half of valid_set\n",
        "valid_subset2_ids = list(range(int(0.8 * n_valid_trials), n_valid_trials))  # Second half of valid_set\n",
        "\n",
        "print(\"Valid Set ids: \" + str(valid_subset1_ids))\n",
        "print(\"------------------------------\")\n",
        "print(\"Test Set ids: \" + str(valid_subset2_ids) + \"\\n\")\n",
        "\n",
        "# Create subsets of valid_set based on indices\n",
        "valid_set_subset = BaseConcatDataset([valid_set_datasets[i] for i in valid_subset1_ids])\n",
        "test_set = BaseConcatDataset([valid_set_datasets[i] for i in valid_subset2_ids])\n",
        "\n",
        "\n",
        "\n",
        "# Update the valid_set with the new subsets\n",
        "valid_set.datasets = valid_set_subset\n",
        "\n",
        "print(\"Valid_set after split: \" + str(get_baseconcatdataset_shape(valid_set_subset)))\n",
        "print(\"------------------------------\")\n",
        "print(\"Test_set after split (N/A before): \" + str(get_baseconcatdataset_shape(test_set)))\n",
        "print(\"------------------------------\")\n",
        "print(\"Train_set (not involved in split): \" + str(get_baseconcatdataset_shape(train_set)))\n"
      ],
      "metadata": {
        "id": "EePemIsPoSaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from braindecode.models import ShallowFBCSPNet\n",
        "from braindecode.util import set_random_seeds\n",
        "\n",
        "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
        "device = 'cuda' if cuda else 'cpu'\n",
        "if cuda:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "# Set random seed to be able to roughly reproduce results\n",
        "# Note that with cudnn benchmark set to True, GPU indeterminism\n",
        "# may still make results substantially different between runs.\n",
        "# To obtain more consistent results at the cost of increased computation time,\n",
        "# you can set `cudnn_benchmark=False` in `set_random_seeds`\n",
        "# or remove `torch.backends.cudnn.benchmark = True`\n",
        "seed = 20200220\n",
        "set_random_seeds(seed=seed, cuda=cuda)\n",
        "\n",
        "n_classes = 4\n",
        "classes = list(range(n_classes))\n",
        "# Extract number of chans and time steps from dataset\n",
        "n_chans = train_set[0][0].shape[0]\n",
        "input_window_samples = train_set[0][0].shape[1]\n",
        "\n",
        "model = ShallowFBCSPNet(\n",
        "    n_chans,\n",
        "    n_classes,\n",
        "    input_window_samples=input_window_samples,\n",
        "    final_conv_length='auto',\n",
        ")\n",
        "\n",
        "# Display torchinfo table describing the model\n",
        "print(model)\n",
        "\n",
        "# Send model to GPU\n",
        "if cuda:\n",
        "    model = model.cuda()"
      ],
      "metadata": {
        "id": "_Lj7_CxfoX6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skorch.callbacks import LRScheduler, EpochScoring\n",
        "from skorch.helper import predefined_split\n",
        "\n",
        "from braindecode import EEGClassifier\n",
        "\n",
        "# We found these values to be good for the shallow network:\n",
        "lr = 0.0625 * 0.01\n",
        "weight_decay = 0\n",
        "\n",
        "# For deep4 they should be:\n",
        "# lr = 1 * 0.01\n",
        "# weight_decay = 0.5 * 0.001\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 50\n",
        "\n",
        "\n",
        "clf = EEGClassifier(\n",
        "    model,\n",
        "    criterion=torch.nn.NLLLoss,\n",
        "    optimizer=torch.optim.AdamW,\n",
        "    train_split=predefined_split(valid_set_subset),  # using valid_set for validation\n",
        "    optimizer__lr=lr,\n",
        "    optimizer__weight_decay=weight_decay,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[\n",
        "        (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
        "        ('train_acc', EpochScoring(scoring='accuracy', on_train=True, name='train_acc')),\n",
        "               ('train_f1', EpochScoring(scoring='f1_macro', on_train=True, name='train_f1')),\n",
        "                ('precision', EpochScoring(scoring='precision_macro', on_train=True, name='train_precision')),\n",
        "                ('recall', EpochScoring(scoring='recall_macro', on_train=True, name='train_recall'))  # Add the F1 score callback\n",
        "\n",
        "    ],\n",
        "    device=device,\n",
        "    classes=classes,\n",
        ")\n",
        "# Model training for the specified number of epochs. `y` is None as it is\n",
        "# already supplied in the dataset.\n",
        "_ = clf.fit(train_set, y=None, epochs=n_epochs)"
      ],
      "metadata": {
        "id": "80WjFbg_oiiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from numpy import array, arange\n",
        "from matplotlib import cm\n",
        "\n",
        "# Extract the values for the metrics\n",
        "train_loss = clf.history_[:, 'train_loss']\n",
        "valid_loss = clf.history_[:, 'valid_loss']\n",
        "train_acc = clf.history_[:, 'train_acc']\n",
        "valid_acc = clf.history_[:, 'valid_acc']\n",
        "train_f1 = clf.history_[:, 'train_f1']\n",
        "precision = clf.history_[:, 'train_precision']\n",
        "recall = clf.history_[:, 'train_recall']\n",
        "\n",
        "# Calculate misclassification rates\n",
        "train_misclass = (100 - 100 * array(train_acc))/100\n",
        "valid_misclass = (100 - 100 * array(valid_acc))/100\n",
        "\n",
        "# Get a colormap\n",
        "cmap = cm.get_cmap('viridis', 10)\n",
        "\n",
        "# Create the plot\n",
        "epochs = arange(len(train_loss))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.plot(epochs, train_loss, label='Training Loss', color=cmap(0))\n",
        "plt.plot(epochs, valid_loss, label='Validation Loss', color=cmap(2))\n",
        "plt.plot(epochs, train_acc, label='Training Accuracy', color=cmap(4))\n",
        "plt.plot(epochs, valid_acc, label='Validation Accuracy', color=cmap(6))\n",
        "plt.plot(epochs, train_f1, label='Training F1 Score', color=cmap(8))\n",
        "plt.plot(epochs, precision, label='Precision', color=cmap(1))\n",
        "plt.plot(epochs, recall, label='Recall', color=cmap(3))\n",
        "plt.plot(epochs, train_misclass, label='Training Misclassification Rate', color=cmap(5))\n",
        "plt.plot(epochs, valid_misclass, label='Validation Misclassification Rate', color=cmap(7))\n",
        "\n",
        "plt.text(epochs[-1], train_loss[-1], 'Train Loss', ha='left', va='center')\n",
        "plt.text(epochs[-1], valid_loss[-1], 'Val. Loss', ha='left', va='center')\n",
        "plt.text(epochs[-1], train_acc[-1], 'Train Acc.', ha='left', va='bottom')\n",
        "plt.text(epochs[-1], valid_acc[-1], 'Val. Acc.', ha='left', va='center')\n",
        "plt.text(epochs[-1], train_f1[-1], 'Train F1', ha='left', va='top')\n",
        "plt.text(epochs[-1], precision[-1], 'Train Precision', ha='right', va='top')\n",
        "plt.text(epochs[-1], recall[-1], 'Train Recall', ha='right', va='bottom')\n",
        "plt.text(epochs[-1], train_misclass[-1], 'Val. Misclass. Rate', ha='left', va='center')\n",
        "plt.text(epochs[-1], valid_misclass[-1], 'Train Misclass. Rate', ha='left', va='center')\n",
        "\n",
        "\n",
        "plt.ylim(0, 2)\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Value (%)')\n",
        "plt.title('Training and Validation Metrics')\n",
        "plt.legend()\n",
        "plt.grid(True, )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WxxYu33lpVzu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
