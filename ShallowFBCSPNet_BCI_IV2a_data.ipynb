{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOT8PMg0imKbS4B3rp1p6EW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickelodeon20/Test-Model-Performances/blob/main/ShallowFBCSPNet_BCI_IV2a_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Authors: Lukas Gemein <l.gemein@gmail.com>\n",
        "#          Hubert Banville <hubert.jbanville@gmail.com>\n",
        "#          Simon Brandt <simonbrandt@protonmail.com>\n",
        "#          Daniel Wilson <dan.c.wil@gmail.com>\n",
        "#\n",
        "# License: BSD (3-clause)\n",
        "!pip install braindecode # Install braindecode package\n",
        "from braindecode.datasets import MOABBDataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f6pFLVf4usf",
        "outputId": "4ae6a85b-849e-486b-e779-9c597308f181"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: braindecode in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.13.0)\n",
            "Requirement already satisfied: skorch in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.1.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from braindecode) (0.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.4.2)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.8.0)\n",
            "Requirement already satisfied: docstring-inheritance in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (2.8.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne->braindecode) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne->braindecode) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne->braindecode) (0.4)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne->braindecode) (1.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne->braindecode) (4.67.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode) (1.5.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->braindecode) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->braindecode) (1.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne->braindecode) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne->braindecode) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->braindecode) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->skorch->braindecode) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne->braindecode) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->braindecode) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->braindecode) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->braindecode) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->braindecode) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subject_id = 3\n",
        "\n",
        "!pip install moabb\n",
        "dataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[subject_id])\n"
      ],
      "metadata": {
        "id": "Y7vUTG0440PA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "405bd0d9-c384-4333-c8a7-96e53e911f1c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moabb in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: PyYAML<7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from moabb) (6.0.2)\n",
            "Requirement already satisfied: coverage<8.0.0,>=7.0.1 in /usr/local/lib/python3.11/dist-packages (from moabb) (7.8.0)\n",
            "Requirement already satisfied: edfio<0.5.0,>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.4.8)\n",
            "Requirement already satisfied: edflib-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.0.8)\n",
            "Requirement already satisfied: h5py<4.0.0,>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from moabb) (3.13.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from moabb) (3.10.0)\n",
            "Requirement already satisfied: memory-profiler<0.62.0,>=0.61.0 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.61.0)\n",
            "Requirement already satisfied: mne<2.0.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.9.0)\n",
            "Requirement already satisfied: mne-bids>=0.14 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.16.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.22 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from moabb) (2.2.2)\n",
            "Requirement already satisfied: pooch<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.8.2)\n",
            "Requirement already satisfied: pyriemann<0.8,>=0.7 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.11/dist-packages (from moabb) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn<1.6 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.5.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.14.1)\n",
            "Requirement already satisfied: seaborn<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from moabb) (0.12.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from moabb) (4.67.1)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.26.15 in /usr/local/lib/python3.11/dist-packages (from moabb) (1.26.20)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.6.2->moabb) (2.8.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from memory-profiler<0.62.0,>=0.61.0->moabb) (5.9.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne<2.0.0,>=1.7.0->moabb) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne<2.0.0,>=1.7.0->moabb) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne<2.0.0,>=1.7.0->moabb) (0.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.2->moabb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.2->moabb) (2025.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch<2.0.0,>=1.6.0->moabb) (4.3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from pyriemann<0.8,>=0.7->moabb) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->moabb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->moabb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.1->moabb) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6->moabb) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.6.2->moabb) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne<2.0.0,>=1.7.0->moabb) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import multiply\n",
        "\n",
        "from braindecode.preprocessing import (Preprocessor,\n",
        "                                       exponential_moving_standardize,\n",
        "                                       preprocess)\n",
        "\n",
        "low_cut_hz = 4.  # low cut frequency for filtering\n",
        "high_cut_hz = 38.  # high cut frequency for filtering\n",
        "# Parameters for exponential moving standardization\n",
        "factor_new = 1e-3\n",
        "init_block_size = 1000\n",
        "# Factor to convert from V to uV\n",
        "factor = 1e6\n",
        "\n",
        "# Define a named function for volt-to-microvolt conversion\n",
        "def convert_to_microvolts(data, factor=factor):\n",
        "    return multiply(data, factor)\n",
        "\n",
        "preprocessors = [\n",
        "    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n",
        "    Preprocessor(convert_to_microvolts,  apply_on_array=True),  # Convert from V to uV using the named function    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n",
        "    Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n",
        "                 factor_new=factor_new, init_block_size=init_block_size)\n",
        "]\n",
        "\n",
        "# Transform the data\n",
        "preprocess(dataset, preprocessors, n_jobs=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2-enRnj8N8C",
        "outputId": "ba7259ea-b50d-4898-bc4d-c96a666dd2b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<braindecode.datasets.moabb.MOABBDataset at 0x7cdd1c6da490>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from braindecode.preprocessing import create_windows_from_events\n",
        "\n",
        "trial_start_offset_seconds = -0.5\n",
        "# Extract sampling frequency, check that they are same in all datasets\n",
        "sfreq = dataset.datasets[0].raw.info['sfreq']\n",
        "assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
        "# Calculate the trial start offset in samples.\n",
        "trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
        "\n",
        "# Create windows using braindecode function for this. It needs parameters to define how\n",
        "# trials should be used.\n",
        "windows_dataset = create_windows_from_events(\n",
        "    dataset,\n",
        "    trial_start_offset_samples=trial_start_offset_samples,\n",
        "    trial_stop_offset_samples=0,\n",
        "    preload=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmBV_1x_8gdk",
        "outputId": "34d80e6c-5876-49c7-889f-54a8f858a186"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from braindecode.datasets import BaseConcatDataset\n",
        "\n",
        "# Split the dataset into training and validation set.\n",
        "\n",
        "splitted = windows_dataset.split('session')\n",
        "train_set = splitted['0train']  # Session train\n",
        "#print(train_set[0][0])\n",
        "print(\"------------------------------\")\n",
        "valid_set = splitted['1test']  # Session evaluation\n",
        "#print(valid_set[0][0])\n",
        "print(\"------------------------------\")\n",
        "\n",
        "\n",
        "# Get the list of datasets within valid_set\n",
        "valid_set_datasets = valid_set.datasets\n",
        "\n",
        "# Create new split indices within the valid_set\n",
        "n_valid_trials = len(valid_set_datasets)\n",
        "valid_subset1_ids = list(range(0, int(0.8 * n_valid_trials)))  # First half of valid_set\n",
        "valid_subset2_ids = list(range(int(0.2 * n_valid_trials), n_valid_trials))  # Second half of valid_set\n",
        "\n",
        "# Create subsets of valid_set based on indices\n",
        "valid_subset1 = BaseConcatDataset([valid_set_datasets[i] for i in valid_subset1_ids])\n",
        "valid_subset2 = BaseConcatDataset([valid_set_datasets[i] for i in valid_subset2_ids])\n",
        "\n",
        "\n",
        "\n",
        "# Update the valid_set with the new subsets\n",
        "valid_set.datasets = [valid_subset1, valid_subset2]\n",
        "\n",
        "valid_set_l, valid_set_w, valid_set_h = valid_set.datasets[0], valid_set.datasets[0][0], valid_set.datasets[0][0][0]\n",
        "print(\"L x W x H = \" + str(len(valid_set_l)) + \" x \" + str(len(valid_set_w)) + \" x \" + str(len(valid_set_h)))\n",
        "print(\"------------------------------\")\n",
        "valid_set_l, valid_set_w, valid_set_h = valid_set.datasets[0], valid_set.datasets[0][0], valid_set.datasets[0][0][0]\n",
        "print(\"L x W x H = \" + str(len(valid_set_l)) + \" x \" + str(len(valid_set_w)) + \" x \" + str(len(valid_set_h)))\n",
        "print(\"------------------------------\")\n",
        "valid_set_l, valid_set_w, valid_set_h = valid_set.datasets[0], valid_set.datasets[0][0], valid_set.datasets[0][0][0]\n",
        "print(\"L x W x H = \" + str(len(valid_set_l)) + \" x \" + str(len(valid_set_w)) + \" x \" + str(len(valid_set_h)))\n",
        "\"\"\"\n",
        "test_size = (len(valid_set.datasets)) // 3\n",
        "\n",
        "# Create the test set using slicing on the underlying datasets\n",
        "test_set = valid_set.datasets[:test_size]\n",
        "test_set = type(valid_set)(test_set) # To maintain BaseConcatDataset type\n",
        "print(test_set[0][0])\n",
        "print(\"------------------------------\")\n",
        "\n",
        "# Update the validation set by excluding the test set portion\n",
        "valid_set = valid_set.datasets[test_size:]\n",
        "valid_set = type(valid_set)(valid_set)  # To maintain BaseConcatDataset type\n",
        "print(valid_set[0][0])\n",
        "print(\"------------------------------\")\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "c-ekf3k48j7q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "d53c0adc-6bb5-41e2-96b1-b518fe63ac8e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "------------------------------\n",
            "[[ 0.22230309 -0.04833731 -1.2568867  ...  0.85724396  2.1186037\n",
            "   0.48831838]\n",
            " [ 0.17376888  0.14103435 -1.4208701  ...  0.7195906   2.7172763\n",
            "   1.7720041 ]\n",
            " [ 0.18767089 -0.09634947 -1.2768091  ...  0.8923618   2.3300319\n",
            "   0.99394417]\n",
            " ...\n",
            " [ 0.70805854  0.5330354  -0.79146415 ...  0.59256047  2.0856087\n",
            "   0.6110683 ]\n",
            " [ 0.61362165  0.3323485  -0.9576285  ...  0.6219022   2.2601748\n",
            "   0.8375986 ]\n",
            " [ 0.657738    0.45626017 -0.7251323  ...  0.47373986  1.8942503\n",
            "   0.72885215]]\n",
            "------------------------------\n",
            "[[-0.76391214 -0.85032845 -1.2509857  ... -0.48516664 -0.13433345\n",
            "  -0.19352888]\n",
            " [-1.07877    -1.173073   -1.6210124  ... -0.8607335  -0.26098868\n",
            "  -0.74236536]\n",
            " [-0.8934356  -1.057702   -1.4941813  ... -0.9723431  -0.5593034\n",
            "  -0.79070634]\n",
            " ...\n",
            " [-0.57635486 -0.4676753  -1.4729612  ... -0.30570632  0.32585996\n",
            "  -0.10835916]\n",
            " [-0.4150864  -0.3560559  -1.1395516  ... -0.30622593  0.43472254\n",
            "   0.01227009]\n",
            " [-0.5184523  -0.3782055  -1.1996509  ... -0.4518671   0.4454043\n",
            "  -0.14711368]]\n",
            "------------------------------\n",
            "L x W x H = 192 x 3 x 22\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_size = (len(valid_set.datasets)) // 3\\n\\n# Create the test set using slicing on the underlying datasets\\ntest_set = valid_set.datasets[:test_size]\\ntest_set = type(valid_set)(test_set) # To maintain BaseConcatDataset type\\nprint(test_set[0][0])\\nprint(\"------------------------------\")\\n\\n# Update the validation set by excluding the test set portion\\nvalid_set = valid_set.datasets[test_size:]\\nvalid_set = type(valid_set)(valid_set)  # To maintain BaseConcatDataset type\\nprint(valid_set[0][0])\\nprint(\"------------------------------\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from braindecode.models import ShallowFBCSPNet\n",
        "from braindecode.util import set_random_seeds\n",
        "\n",
        "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
        "device = 'cuda' if cuda else 'cpu'\n",
        "if cuda:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "# Set random seed to be able to roughly reproduce results\n",
        "# Note that with cudnn benchmark set to True, GPU indeterminism\n",
        "# may still make results substantially different between runs.\n",
        "# To obtain more consistent results at the cost of increased computation time,\n",
        "# you can set `cudnn_benchmark=False` in `set_random_seeds`\n",
        "# or remove `torch.backends.cudnn.benchmark = True`\n",
        "seed = 20200220\n",
        "set_random_seeds(seed=seed, cuda=cuda)\n",
        "\n",
        "n_classes = 4\n",
        "classes = list(range(n_classes))\n",
        "# Extract number of chans and time steps from dataset\n",
        "n_chans = train_set[0][0].shape[0]\n",
        "input_window_samples = train_set[0][0].shape[1]\n",
        "\n",
        "model = ShallowFBCSPNet(\n",
        "    n_chans,\n",
        "    n_classes,\n",
        "    input_window_samples=input_window_samples,\n",
        "    final_conv_length='auto',\n",
        ")\n",
        "\n",
        "# Display torchinfo table describing the model\n",
        "print(model)\n",
        "\n",
        "# Send model to GPU\n",
        "if cuda:\n",
        "    model = model.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPc-XIBx88uC",
        "outputId": "56ed4a7e-08eb-4d0c-f75c-f4664f01518d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================================================================\n",
            "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
            "============================================================================================================================================\n",
            "ShallowFBCSPNet (ShallowFBCSPNet)        [1, 22, 1125]             [1, 4]                    --                        --\n",
            "├─Ensure4d (ensuredims): 1-1             [1, 22, 1125]             [1, 22, 1125, 1]          --                        --\n",
            "├─Rearrange (dimshuffle): 1-2            [1, 22, 1125, 1]          [1, 1, 1125, 22]          --                        --\n",
            "├─CombinedConv (conv_time_spat): 1-3     [1, 1, 1125, 22]          [1, 40, 1101, 1]          36,240                    --\n",
            "├─BatchNorm2d (bnorm): 1-4               [1, 40, 1101, 1]          [1, 40, 1101, 1]          80                        --\n",
            "├─Expression (conv_nonlin_exp): 1-5      [1, 40, 1101, 1]          [1, 40, 1101, 1]          --                        --\n",
            "├─AvgPool2d (pool): 1-6                  [1, 40, 1101, 1]          [1, 40, 69, 1]            --                        [75, 1]\n",
            "├─Expression (pool_nonlin_exp): 1-7      [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
            "├─Dropout (drop): 1-8                    [1, 40, 69, 1]            [1, 40, 69, 1]            --                        --\n",
            "├─Sequential (final_layer): 1-9          [1, 40, 69, 1]            [1, 4]                    --                        --\n",
            "│    └─Conv2d (conv_classifier): 2-1     [1, 40, 69, 1]            [1, 4, 1, 1]              11,044                    [69, 1]\n",
            "│    └─LogSoftmax (logsoftmax): 2-2      [1, 4, 1, 1]              [1, 4, 1, 1]              --                        --\n",
            "│    └─Expression (squeeze): 2-3         [1, 4, 1, 1]              [1, 4]                    --                        --\n",
            "============================================================================================================================================\n",
            "Total params: 47,364\n",
            "Trainable params: 47,364\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.MEGABYTES): 0.01\n",
            "============================================================================================================================================\n",
            "Input size (MB): 0.10\n",
            "Forward/backward pass size (MB): 0.35\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.50\n",
            "============================================================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/braindecode/models/base.py:23: UserWarning: ShallowFBCSPNet: 'input_window_samples' is depreciated. Use 'n_times' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/braindecode/models/base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
            "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from skorch.callbacks import LRScheduler\n",
        "from skorch.helper import predefined_split\n",
        "\n",
        "from braindecode import EEGClassifier\n",
        "\n",
        "# We found these values to be good for the shallow network:\n",
        "lr = 0.0625 * 0.01\n",
        "weight_decay = 0\n",
        "\n",
        "# For deep4 they should be:\n",
        "# lr = 1 * 0.01\n",
        "# weight_decay = 0.5 * 0.001\n",
        "\n",
        "batch_size = 64\n",
        "n_epochs = 50\n",
        "\n",
        "clf = EEGClassifier(\n",
        "    model,\n",
        "    criterion=torch.nn.NLLLoss,\n",
        "    optimizer=torch.optim.AdamW,\n",
        "    train_split=predefined_split(valid_set),  # using valid_set for validation\n",
        "    optimizer__lr=lr,\n",
        "    optimizer__weight_decay=weight_decay,\n",
        "    batch_size=batch_size,\n",
        "    callbacks=[\n",
        "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
        "    ],\n",
        "    device=device,\n",
        "    classes=classes,\n",
        ")\n",
        "# Model training for the specified number of epochs. `y` is None as it is\n",
        "# already supplied in the dataset.\n",
        "_ = clf.fit(train_set, y=None, epochs=n_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ER75JxNP9A95",
        "outputId": "693c62a5-159f-4b46-8374-0ad850e086b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'skorch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-53b9d607c8d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLRScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpredefined_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbraindecode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEEGClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skorch'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Line Graph"
      ],
      "metadata": {
        "id": "m4-twsnLLsQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "# Extract loss and accuracy values for plotting from history object\n",
        "results_columns = ['train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy']\n",
        "df = pd.DataFrame(clf.history[:, results_columns], columns=results_columns,\n",
        "                  index=clf.history[:, 'epoch'])\n",
        "\n",
        "# get percent of misclass for better visual comparison to loss\n",
        "df = df.assign(train_misclass=100 - 100 * df.train_accuracy,\n",
        "               valid_misclass=100 - 100 * df.valid_accuracy)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(8, 3))\n",
        "df.loc[:, ['train_loss', 'valid_loss']].plot(\n",
        "    ax=ax1, style=['-', ':'], marker='o', color='tab:blue', legend=False, fontsize=14)\n",
        "\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\n",
        "ax1.set_ylabel(\"Loss\", color='tab:blue', fontsize=14)\n",
        "\n",
        "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "df.loc[:, ['train_misclass', 'valid_misclass']].plot(\n",
        "    ax=ax2, style=['-', ':'], marker='o', color='tab:red', legend=False)\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\n",
        "ax2.set_ylabel(\"Misclassification Rate [%]\", color='tab:red', fontsize=14)\n",
        "ax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend\n",
        "ax1.set_xlabel(\"Epoch\", fontsize=14)\n",
        "\n",
        "# where some data has already been plotted to ax\n",
        "handles = []\n",
        "handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle='-', label='Train'))\n",
        "handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle=':', label='Valid'))\n",
        "plt.legend(handles, [h.get_label() for h in handles], fontsize=14)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "LTdxBJI49aNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Confusion Matrices\n"
      ],
      "metadata": {
        "id": "zSvZQ_JuLm46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from braindecode.visualization import plot_confusion_matrix\n",
        "\n",
        "# generate confusion matrices\n",
        "# get the targets\n",
        "y_true = valid_set.get_metadata().target\n",
        "y_pred = clf.predict(valid_set)\n",
        "\n",
        "# generating confusion matrix\n",
        "confusion_mat = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# add class labels\n",
        "# label_dict is class_name : str -> i_class : int\n",
        "label_dict = windows_dataset.datasets[0].window_kwargs[0][1]['mapping']\n",
        "# sort the labels by values (values are integer class labels)\n",
        "labels = [k for k, v in sorted(label_dict.items(), key=lambda kv: kv[1])]\n",
        "\n",
        "# plot the basic conf. matrix\n",
        "plot_confusion_matrix(confusion_mat, class_names=labels)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "pNGJJuWR9eCi",
        "outputId": "92dfde1e-1d2e-4fd6-9b51-983d7d90977e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'braindecode'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-25740d2be449>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbraindecode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# generate confusion matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'braindecode'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Confusion Matrices"
      ],
      "metadata": {
        "id": "8FRRMd1WLjx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate confusion matrices\n",
        "# get the targets\n",
        "y_true_test = test_set.get_metadata().target\n",
        "y_pred_test = clf.predict(test_set)\n",
        "\n",
        "# generating confusion matrix\n",
        "confusion_mat_test = confusion_matrix(y_true_test, y_pred_test)\n",
        "\n",
        "# add class labels\n",
        "# label_dict is class_name : str -> i_class : int\n",
        "label_dict = windows_dataset.datasets[0].window_kwargs[0][1]['mapping']\n",
        "# sort the labels by values (values are integer class labels)\n",
        "labels = [k for k, v in sorted(label_dict.items(), key=lambda kv: kv[1])]\n",
        "\n",
        "# plot the basic conf. matrix\n",
        "plot_confusion_matrix(confusion_mat_test, class_names=labels"
      ],
      "metadata": {
        "id": "29Nt2HuGLZ9_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}